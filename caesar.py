#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CAESAR CIPHER CRACKER ‚Äî ULTIMATE EDITION
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–π —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏:
  1. Chi-squared —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ä–µ–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
  2. –ë–∏–≥—Ä–∞–º–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤)
  3. Index of Coincidence (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω –ª–∏ —Ç–µ–∫—Å—Ç?)
  4. –°–ª–æ–≤–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º —Å—Ç–µ–º–º–∏–Ω–≥–æ–º
  5. –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —à–∏—Ñ—Ä–æ–≤
  6. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
"""

import sys
import re
import math
import argparse
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Set
from dataclasses import dataclass, field
from functools import lru_cache
from collections import Counter

try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
    from rich import box
    from rich.prompt import Prompt, Confirm
    from rich.text import Text
    HAS_RICH = True
except ImportError:
    HAS_RICH = False


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –õ–ò–ù–ì–í–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ö–û–ù–°–¢–ê–ù–¢–´
# –†–µ–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ù–ö–†–Ø)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

RU_ALPHA = '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
RU_SET   = frozenset(RU_ALPHA)
RU_SIZE  = len(RU_ALPHA)  # 33

EN_ALPHA = 'abcdefghijklmnopqrstuvwxyz'
EN_SET   = frozenset(EN_ALPHA)
EN_SIZE  = len(EN_ALPHA)  # 26

# –ß–∞—Å—Ç–æ—Ç—ã –±—É–∫–≤ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–ù–ö–†–Ø)
RU_LETTER_FREQ = {
    '–æ': 0.1097, '–µ': 0.0845, '–∞': 0.0801, '–∏': 0.0735, '–Ω': 0.0670,
    '—Ç': 0.0626, '—Å': 0.0547, '—Ä': 0.0473, '–≤': 0.0454, '–ª': 0.0440,
    '–∫': 0.0349, '–º': 0.0321, '–¥': 0.0298, '–ø': 0.0281, '—É': 0.0262,
    '—è': 0.0201, '—ã': 0.0190, '—å': 0.0174, '–≥': 0.0170, '–∑': 0.0165,
    '–±': 0.0159, '—á': 0.0144, '–π': 0.0121, '—Ö': 0.0097, '–∂': 0.0094,
    '—à': 0.0073, '—é': 0.0064, '—Ü': 0.0048, '—â': 0.0036, '—ç': 0.0032,
    '—Ñ': 0.0026, '—ä': 0.0004, '—ë': 0.0004,
}

# –ß–∞—Å—Ç–æ—Ç—ã –±—É–∫–≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (Cornell)
EN_LETTER_FREQ = {
    'e': 0.1270, 't': 0.0906, 'a': 0.0817, 'o': 0.0751, 'i': 0.0697,
    'n': 0.0675, 's': 0.0633, 'h': 0.0609, 'r': 0.0599, 'd': 0.0425,
    'l': 0.0403, 'c': 0.0278, 'u': 0.0276, 'm': 0.0241, 'w': 0.0236,
    'f': 0.0223, 'g': 0.0202, 'y': 0.0197, 'p': 0.0193, 'b': 0.0129,
    'v': 0.0098, 'k': 0.0077, 'j': 0.0015, 'x': 0.0015, 'q': 0.0010,
    'z': 0.0007,
}

# –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –±–∏–≥—Ä–∞–º–º—ã
RU_COMMON_BIGRAMS = frozenset({
    '—Å—Ç', '–Ω–æ', '—Ç–æ', '–Ω–∞', '–µ–Ω', '–Ω–∏', '–∫–æ', '—Ä–∞', '–æ–≤', '—Ä–æ',
    '–æ—Å', '–∞–ª', '–µ—Ä', '–æ–Ω', '–Ω–µ', '–ª–∏', '–ø–æ', '—Ä–µ', '–æ—Ä', '–∞–Ω',
    '–ø—Ä', '–µ—Ç', '–æ–ª', '—Ç–∞', '–µ–ª', '–∫–∞', '–≤–æ', '—Ç–∏', '–≤–∞', '–æ–¥',
    '–∞—Ç', '–ª–µ', '–æ—Ç', '—Ç–µ', '–ª–∞', '–æ–º', '–¥–µ', '–µ—Å', '–≤–µ', '–ª–æ',
    '–æ–≥', '–∑–∞', '—Å–∫', '—Ç—å', '–∏–Ω', '–∏—Ç', '–ø–µ', '—Å–µ', '–æ–±', '–¥–∞',
    '–µ–º', '–≥–æ', '–∞—Å', '–∏–∑', '–∏–µ', '—Ä–∏', '–∏–ª', '–µ–¥', '–∞—Ä', '–∞–º',
    '–¥–æ', '–∏—Å', '—Ç—Ä', '–Ω—ã', '–º–∏', '—á–∞', '–±–æ', '–æ—Ä', '–µ–≥', '—Ä—É',
    '–º–µ', '–º–æ', '–≥–∏', '–¥–∏', '–≤–∏', '–±–µ', '–∞–∫', '–∫–∏', '–æ–µ', '—ë–º',
})

EN_COMMON_BIGRAMS = frozenset({
    'th', 'he', 'in', 'er', 'an', 're', 'on', 'at', 'en', 'nd',
    'ti', 'es', 'or', 'te', 'of', 'ed', 'is', 'it', 'al', 'ar',
    'st', 'to', 'nt', 'ng', 'se', 'ha', 'as', 'ou', 'io', 'le',
    've', 'co', 'me', 'de', 'hi', 'ri', 'ro', 'ic', 'ne', 'ea',
    'ra', 'ce', 'li', 'ch', 'll', 'be', 'ma', 'si', 'om', 'ur',
    'ca', 'el', 'ta', 'la', 'ns', 'ge', 'ha', 'ec', 'it', 'il',
    'pe', 'ol', 'no', 'na', 'us', 'di', 'wa', 'em', 'ac', 'ss',
})

# –°—É—Ñ—Ñ–∏–∫—Å—ã –¥–ª—è —Å—Ç–µ–º–º–∏–Ω–≥–∞
RU_SUFFIXES = (
    '–æ—Å—Ç—å', '–µ–Ω–∏–µ', '–∞–Ω–∏–µ', '—Ç—å—Å—è', '—é—Ç—Å—è', '–∏—Ç—Å—è', '–Ω–æ–≥–æ', '–Ω–æ–º—É',
    '—Å–∫–∏–º', '—Å–∫–æ–π', '–Ω—ã—Ö', '–Ω—ã–µ', '–Ω—ã–π', '–Ω–∞—è', '–Ω–æ–µ', '–Ω–æ–π',
    '–æ–≥–æ', '–æ–º—É', '—ã–º–∏', '–∞–º–∏', '—è–º–∏', '–∞—Ç—å', '—è—Ç—å', '–µ—Ç—å', '–∏—Ç—å',
    '—É–µ—Ç', '–∞–µ—Ç', '—ë—Ç', '—é—Ç', '—É—Ç', '–∏—Ç', '–µ—Ç',
    '–æ–≤', '–µ–≤', '–µ–π', '–∏–π', '—ã–π', '–æ–π', '–∞—è', '–æ–µ', '–∏–µ',
    '–æ–º', '–µ–º', '–∞–º', '—è–º', '–∞—Ö', '—è—Ö', '—ã—Ö', '–∏—Ö',
    '–∞–ª', '–∏–ª', '–µ–ª', '–æ–ª', '—É–ª',
    '—Ç—å', '—Å—è', '—Å—å',
)

EN_SUFFIXES = (
    'tion', 'ness', 'ment', 'able', 'ible', 'ious', 'eous',
    'ing', 'ous', 'ful', 'ive', 'ity', 'ent', 'ant', 'ion',
    'ism', 'ist', 'ory', 'ary', 'ery', 'ure', 'age', 'ise', 'ize',
    'ly', 'er', 'ed', 'es', 'al', 'en', 'ty',
    'or', 'ic', 'le',
    's',
)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATA CLASSES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass(frozen=True)
class ShiftResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞"""
    shift: int
    text: str
    chi_sq: float          # Chi-squared (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
    bigram_score: float    # –ë–∏–≥—Ä–∞–º–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ [0..1]
    dict_score: float      # –°–ª–æ–≤–∞—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ [0..1]
    stem_score: float      # –°—Ç–µ–º–º–∏–Ω–≥-–æ—Ü–µ–Ω–∫–∞ [0..1]
    combined: float        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ [0..1]
    matches: int           # –°–ª–æ–≤–∞—Ä–Ω—ã—Ö —Å–ª–æ–≤
    total_words: int       # –í—Å–µ–≥–æ —Å–ª–æ–≤

    @property
    def confidence(self) -> float:
        return min(self.combined * 100, 100.0)


@dataclass(frozen=True)
class Segment:
    """–°–µ–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —à–∏—Ñ—Ä–∞)"""
    text: str
    start: int
    end: int
    best_result: ShiftResult


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –°–õ–û–í–ê–†–¨
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

_SCRIPT_DIR = Path(__file__).resolve().parent


class Dictionary:
    """–°–∏–Ω–≥–ª—Ç–æ–Ω-—Å–ª–æ–≤–∞—Ä—å —Å –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π (—Ä—É—Å—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)"""
    _inst = None
    _ru_words: Optional[Set[str]] = None
    _en_words: Optional[Set[str]] = None

    def __new__(cls):
        if cls._inst is None:
            cls._inst = super().__new__(cls)
        return cls._inst

    @property
    def ru_words(self) -> Set[str]:
        if self._ru_words is None:
            self._load_ru()
        return self._ru_words

    @property
    def en_words(self) -> Set[str]:
        if self._en_words is None:
            self._load_en()
        return self._en_words

    def words(self, lang: str) -> Set[str]:
        return self.ru_words if lang == 'ru' else self.en_words

    @staticmethod
    def _find(name: str) -> Optional[Path]:
        """1. –†—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º  2. CWD  3. HOME"""
        for p in [_SCRIPT_DIR / name, Path(name), Path.home() / name]:
            if p.exists() and p.stat().st_size > 100:
                return p
        return None

    def _load_file(self, path: Path) -> Set[str]:
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                return {
                    w.lower() for line in f
                    if (w := line.strip()) and 2 <= len(w) <= 50 and w.isalpha()
                }
        except Exception:
            return set()

    def _load_ru(self):
        p = self._find('russian_dict.txt')
        self._ru_words = self._load_file(p) if p else set()
        self._ru_words |= {
            '–∏', '–≤', '–Ω–µ', '–Ω–∞', '–æ–Ω', '—á—Ç–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',
            '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã',
            '–ø–æ', '–æ—Ç', '–∏–∑', '–¥–ª—è', '—ç—Ç–æ', '–º—ã', '–æ–Ω–∏', '–±—ã–ª', '–±—ã—Ç—å',
        }

    def _load_en(self):
        p = self._find('english_dict.txt')
        self._en_words = self._load_file(p) if p else set()
        self._en_words |= {
            'the', 'be', 'to', 'of', 'and', 'in', 'that', 'have', 'it', 'for',
            'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but',
            'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or', 'an',
            'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what', 'so',
            'if', 'about', 'who', 'get', 'which', 'go', 'when', 'can', 'no',
        }

    def __len__(self) -> int:
        return len(self.ru_words) + len(self.en_words)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–®–ò–§–†–û–í–©–ò–ö
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Decryptor:
    """–î–µ—à–∏—Ñ—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ str.translate() ‚Äî O(n), —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ C"""

    @staticmethod
    @lru_cache(maxsize=128)
    def _table(shift: int, lang: str) -> dict:
        alpha = RU_ALPHA if lang == 'ru' else EN_ALPHA
        size = len(alpha)
        lo = alpha
        up = lo.upper()
        s_lo = ''.join(lo[(i - shift) % size] for i in range(size))
        s_up = ''.join(up[(i - shift) % size] for i in range(size))
        return str.maketrans(lo + up, s_lo + s_up)

    @staticmethod
    def decrypt(text: str, shift: int, lang: str = 'ru') -> str:
        return text.translate(Decryptor._table(shift, lang))


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –°–ö–û–†–ï–†–´ (–º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def chi_squared(text: str, lang: str = 'ru') -> float:
    """
    Chi-squared —Ç–µ—Å—Ç: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –±—É–∫–≤ —Å —ç—Ç–∞–ª–æ–Ω–æ–º.
    –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ.
    """
    charset = RU_SET if lang == 'ru' else EN_SET
    freqs = RU_LETTER_FREQ if lang == 'ru' else EN_LETTER_FREQ

    letters = [c for c in text.lower() if c in charset]
    n = len(letters)
    if n == 0:
        return float('inf')

    observed = Counter(letters)
    chi_sq = 0.0
    for char, expected_freq in freqs.items():
        expected = expected_freq * n
        actual = observed.get(char, 0)
        if expected > 0:
            chi_sq += (actual - expected) ** 2 / expected

    return chi_sq


def bigram_score(text: str, lang: str = 'ru') -> float:
    """–û—Ü–µ–Ω–∫–∞ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º."""
    charset = RU_SET if lang == 'ru' else EN_SET
    common = RU_COMMON_BIGRAMS if lang == 'ru' else EN_COMMON_BIGRAMS

    letters = [c for c in text.lower() if c in charset]
    if len(letters) < 4:
        return 0.0

    bigrams = [letters[i] + letters[i+1] for i in range(len(letters) - 1)]
    if not bigrams:
        return 0.0

    hits = sum(1 for bg in bigrams if bg in common)
    return hits / len(bigrams)


def index_of_coincidence(text: str, lang: str = 'ru') -> float:
    """
    Index of Coincidence.
    RU ‚âà 0.0553, EN ‚âà 0.0667, random_ru ‚âà 0.0303, random_en ‚âà 0.0385
    """
    charset = RU_SET if lang == 'ru' else EN_SET

    letters = [c for c in text.lower() if c in charset]
    n = len(letters)
    if n < 2:
        return 0.0

    freq = Counter(letters)
    ic = sum(f * (f - 1) for f in freq.values()) / (n * (n - 1))
    return ic


def stem_word(word: str, lang: str = 'ru') -> str:
    """–õ—ë–≥–∫–∏–π —Å—Ç–µ–º–º–∏–Ω–≥: –æ—Ç—Ä–µ–∑–∞–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å—ã."""
    suffixes = RU_SUFFIXES if lang == 'ru' else EN_SUFFIXES
    min_base = 2 if lang == 'en' else 3  # –ê–Ω–≥–ª. –æ—Å–Ω–æ–≤—ã –∫–æ—Ä–æ—á–µ
    for suffix in suffixes:
        if len(word) > len(suffix) + min_base and word.endswith(suffix):
            return word[:-len(suffix)]
    return word


def normalize_yo(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ë‚Üí–µ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ–º—É –Ω–∞–ø–∏—Å–∞–Ω–∏—é"""
    return text.replace('—ë', '–µ').replace('–Å', '–ï')


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –°–õ–û–í–ê–†–ù–´–ô –ê–ù–ê–õ–ò–ó
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

_RU_WORD_RE = re.compile(r'[–∞-—è—ë–ê-–Ø–Å]{2,}')
_EN_WORD_RE = re.compile(r'[a-zA-Z]{2,}')


def extract_words(text: str, lang: str = 'ru') -> Tuple[str, ...]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    pattern = _RU_WORD_RE if lang == 'ru' else _EN_WORD_RE
    return tuple(w.lower() for w in pattern.findall(text))


def dict_score(text: str, dictionary: Set[str], lang: str = 'ru') -> Tuple[float, int, int]:
    """
    –°–ª–æ–≤–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º –ø–æ–∏—Å–∫–æ–º:
    1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    2. –ë–µ–∑ —ë (–µ –≤–º–µ—Å—Ç–æ —ë) [—Ç–æ–ª—å–∫–æ RU]
    3. –°—Ç–µ–º–º–∏–Ω–≥ + –ø–æ–∏—Å–∫
    """
    words = extract_words(text, lang)
    if not words:
        return 0.0, 0, 0

    matches = 0
    match_weight = 0
    total_weight = 0

    for word in words:
        wlen = len(word)
        total_weight += wlen

        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if word in dictionary:
            matches += 1
            match_weight += wlen
            continue

        # 2. –ó–∞–º–µ–Ω–∞ —ë‚Üí–µ (—Ç–æ–ª—å–∫–æ RU)
        if lang == 'ru':
            word_no_yo = normalize_yo(word)
            if word_no_yo != word and word_no_yo in dictionary:
                matches += 1
                match_weight += wlen
                continue
        else:
            word_no_yo = word

        # 3. –°—Ç–µ–º–º–∏–Ω–≥
        stem = stem_word(word, lang)
        if stem != word and stem in dictionary:
            matches += 1
            match_weight += wlen * 0.8
            continue

        # 4. –°—Ç–µ–º–º–∏–Ω–≥ + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if lang == 'ru':
            stem_no_yo = stem_word(word_no_yo, lang)
            if stem_no_yo != word_no_yo and stem_no_yo in dictionary:
                matches += 1
                match_weight += wlen * 0.7
                continue

    ratio = matches / len(words) if words else 0.0
    weighted = match_weight / total_weight if total_weight > 0 else 0.0

    return ratio * 0.5 + weighted * 0.5, matches, len(words)


def stem_dict_score(text: str, dictionary: Set[str], lang: str = 'ru') -> float:
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å—Ç–µ–º–º–∏–Ω–≥: –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ—Ä–Ω—è."""
    words = extract_words(text, lang)
    if not words:
        return 0.0

    min_stem = 2 if lang == 'en' else 3
    hits = 0
    for word in words:
        stem = stem_word(normalize_yo(word) if lang == 'ru' else word, lang)
        candidate = stem
        while len(candidate) >= min_stem:
            if candidate in dictionary:
                hits += 1
                break
            candidate = candidate[:-1]

    return hits / len(words) if words else 0.0


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–õ–ê–í–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–†
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Analyzer:
    """
    –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏.
    
    –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (>50 –±—É–∫–≤):
      ‚Äî Chi-squared –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç (–æ—á–µ–Ω—å –Ω–∞–¥—ë–∂–µ–Ω)
      ‚Äî –°–ª–æ–≤–∞—Ä—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç
    
    –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (<20 –±—É–∫–≤):
      ‚Äî –ë–∏–≥—Ä–∞–º–º—ã —Å–ø–∞—Å–∞—é—Ç
      ‚Äî –°–ª–æ–≤–∞—Ä—å + —Å—Ç–µ–º–º–∏–Ω–≥ –∫—Ä–∏—Ç–∏—á–Ω—ã
      ‚Äî Chi-squared –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω (–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö)
    
    –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö:
      ‚Äî –ë–∞–ª–∞–Ω—Å –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
    """

    def __init__(self):
        self.dict = Dictionary()

    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
        ru = sum(1 for c in text.lower() if c in RU_SET)
        en = sum(1 for c in text.lower() if c in EN_SET)
        return 'ru' if ru > en else 'en'

    def is_bilingual(self, text: str) -> bool:
        """–ï—Å—Ç—å –ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –æ–±–∞ —è–∑—ã–∫–∞ (–∑–Ω–∞—á–∏–º–æ)"""
        ru = sum(1 for c in text.lower() if c in RU_SET)
        en = sum(1 for c in text.lower() if c in EN_SET)
        total = ru + en
        if total == 0:
            return False
        minor = min(ru, en)
        return minor / total > 0.05  # >5% –º–∏–Ω–æ—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞

    def _letter_count(self, text: str, lang: str = 'ru') -> int:
        charset = RU_SET if lang == 'ru' else EN_SET
        return sum(1 for c in text if c.lower() in charset)

    def analyze_shift(self, text: str, shift: int, lang: str = 'ru') -> ShiftResult:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å–¥–≤–∏–≥–∞"""
        decrypted = Decryptor.decrypt(text, shift, lang)
        dictionary = self.dict.words(lang)

        # 1. Chi-squared
        chi = chi_squared(decrypted, lang)

        # 2. –ë–∏–≥—Ä–∞–º–º—ã
        bg = bigram_score(decrypted, lang)

        # 3. –°–ª–æ–≤–∞—Ä—å
        ds, matches, total = dict_score(decrypted, dictionary, lang)

        # 4. –°—Ç–µ–º–º–∏–Ω–≥
        ss = stem_dict_score(decrypted, dictionary, lang)

        # 5. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        letter_count = self._letter_count(text, lang)
        combined = self._combine(chi, bg, ds, ss, letter_count)

        return ShiftResult(
            shift=shift,
            text=decrypted,
            chi_sq=chi,
            bigram_score=bg,
            dict_score=ds,
            stem_score=ss,
            combined=combined,
            matches=matches,
            total_words=total,
        )

    def _combine(
        self, chi: float, bg: float, ds: float, ss: float, n_letters: int
    ) -> float:
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å–∫–æ—Ä–æ–≤.
        –í–µ—Å–∞ –º–µ–Ω—è—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞.
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chi-squared –≤ [0..1] (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º: –º–µ–Ω—å—à–µ chi = –ª—É—á—à–µ)
        # –¢–∏–ø–∏—á–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: 20-1000
        chi_norm = max(0.0, 1.0 - chi / 500.0)

        if n_letters >= 100:
            # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: chi-squared –æ—á–µ–Ω—å –Ω–∞–¥—ë–∂–µ–Ω
            w_chi, w_bg, w_dict, w_stem = 0.35, 0.10, 0.35, 0.20
        elif n_letters >= 30:
            # –°—Ä–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç: –±–∞–ª–∞–Ω—Å
            w_chi, w_bg, w_dict, w_stem = 0.20, 0.20, 0.35, 0.25
        elif n_letters >= 10:
            # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: –±–∏–≥—Ä–∞–º–º—ã –∏ —Å–ª–æ–≤–∞—Ä—å –≤–∞–∂–Ω–µ–µ
            w_chi, w_bg, w_dict, w_stem = 0.10, 0.30, 0.35, 0.25
        else:
            # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π: –±–∏–≥—Ä–∞–º–º—ã –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç
            w_chi, w_bg, w_dict, w_stem = 0.05, 0.45, 0.30, 0.20

        return w_chi * chi_norm + w_bg * bg + w_dict * ds + w_stem * ss

    def crack(self, text: str, lang: str = None) -> List[ShiftResult]:
        """–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å–¥–≤–∏–≥–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫"""
        if lang is None:
            lang = self.detect_language(text)
        alpha_size = RU_SIZE if lang == 'ru' else EN_SIZE
        results = [self.analyze_shift(text, s, lang) for s in range(alpha_size)]
        results.sort(key=lambda r: r.combined, reverse=True)
        return results

    def is_already_plaintext(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —É–∂–µ –æ—Ç–∫—Ä—ã—Ç—ã–º"""
        lang = self.detect_language(text)
        dictionary = self.dict.words(lang)
        ds, matches, total = dict_score(text, dictionary, lang)

        if total > 0 and matches / total >= 0.7:
            return True

        if self._letter_count(text, lang) >= 30:
            ic = index_of_coincidence(text, lang)
            ic_threshold = 0.045 if lang == 'ru' else 0.055
            return ic > ic_threshold and ds > 0.4

        return False


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –†–ê–ó–ë–ò–ï–ù–ò–ï –ü–û –Ø–ó–´–ö–ê–ú
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dataclass
class LangSegment:
    """–°–µ–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ–¥–Ω–æ–º —è–∑—ã–∫–µ"""
    text: str
    lang: str
    start: int
    end: int


def split_by_language(text: str) -> List[LangSegment]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ —è–∑—ã–∫—É.
    –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ü–∏—Ñ—Ä—ã) –ø—Ä–∏–∫–ª–µ–∏–≤–∞—é—Ç—Å—è –∫ —Ç–µ–∫—É—â–µ–º—É —è–∑—ã–∫—É.
    """
    if not text:
        return []

    segments: List[LangSegment] = []
    cur_lang = None
    cur_start = 0

    for i, ch in enumerate(text):
        cl = ch.lower()
        if cl in RU_SET:
            det = 'ru'
        elif cl in EN_SET:
            det = 'en'
        else:
            continue  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª

        if cur_lang is None:
            cur_lang = det
        elif det != cur_lang:
            # –°–º–µ–Ω–∞ —è–∑—ã–∫–∞ ‚Äî –æ—Ç—Ä–µ–∑–∞–µ–º –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ —Å–ª–æ–≤–∞
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª/\n –ø–µ—Ä–µ–¥ i
            split_at = i
            for j in range(i - 1, max(i - 10, cur_start - 1), -1):
                if text[j] in ' \n\t':
                    split_at = j + 1
                    break
            if split_at > cur_start:
                segments.append(LangSegment(
                    text=text[cur_start:split_at],
                    lang=cur_lang,
                    start=cur_start,
                    end=split_at,
                ))
            cur_start = split_at
            cur_lang = det

    # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
    if cur_start < len(text) and cur_lang:
        segments.append(LangSegment(
            text=text[cur_start:],
            lang=cur_lang,
            start=cur_start,
            end=len(text),
        ))

    return segments if segments else [LangSegment(text=text, lang='ru', start=0, end=len(text))]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–¢–ï–ö–¢–û–† –°–ú–ï–®–ê–ù–ù–´–• –®–ò–§–†–û–í
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class MixedDetector:
    """
    –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü —Å–º–µ–Ω—ã –∫–ª—é—á–∞.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –µ–¥–∏–Ω—ã–π –∫–ª—é—á
    2. –ï—Å–ª–∏ confidence < –ø–æ—Ä–æ–≥–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—É —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —à–∏—Ñ—Ä–∞
    3. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ: –Ω–∞ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤—ã—á–∏—Å–ª—è–µ–º
       –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏
    4. –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫–∏ —Å–º–µ–Ω—ã –∫–ª—é—á–∞ (–≥–¥–µ –∫–ª—é—á –º–µ–Ω—è–µ—Ç—Å—è)
    5. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ —ç—Ç–∏–º —Ç–æ—á–∫–∞–º
    6. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ ‚Äî –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    """

    def __init__(self):
        self.analyzer = Analyzer()
        self.window_size = 40  # –°–∏–º–≤–æ–ª–æ–≤ –≤ –æ–∫–Ω–µ

    def detect(self, text: str) -> List[Segment]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏"""
        lang = self.analyzer.detect_language(text)
        charset = RU_SET if lang == 'ru' else EN_SET
        letters_only = [c for c in text if c.lower() in charset]
        n = len(letters_only)

        if n < self.window_size * 2:
            results = self.analyzer.crack(text, lang)
            best = results[0]
            return [Segment(text=best.text, start=0, end=len(text), best_result=best)]

        shift_map = self._compute_shift_map(text, lang)
        boundaries = self._find_boundaries(shift_map, text)

        segments = []
        for start, end in boundaries:
            segment_text = text[start:end]
            results = self.analyzer.crack(segment_text, lang)
            best = results[0]
            segments.append(Segment(
                text=best.text, start=start, end=end, best_result=best
            ))

        return segments

    def _compute_shift_map(self, text: str, lang: str = 'ru') -> List[int]:
        """–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–ª—é—á —á–µ—Ä–µ–∑ –æ–∫–Ω–æ"""
        charset = RU_SET if lang == 'ru' else EN_SET
        alpha_size = RU_SIZE if lang == 'ru' else EN_SIZE
        n = len(text)
        shift_map = []
        half_w = self.window_size // 2

        for i in range(n):
            if text[i].lower() not in charset:
                shift_map.append(shift_map[-1] if shift_map else 0)
                continue

            start = max(0, i - half_w)
            end = min(n, i + half_w)
            window = text[start:end]

            best_shift = 0
            best_score = -1.0

            for s in range(alpha_size):
                dec = Decryptor.decrypt(window, s, lang)
                chi = chi_squared(dec, lang)
                bg = bigram_score(dec, lang)
                score = bg * 0.6 + max(0, 1 - chi / 500) * 0.4

                if score > best_score:
                    best_score = score
                    best_shift = s

            shift_map.append(best_shift)

        return shift_map

    def _find_boundaries(
        self, shift_map: List[int], text: str
    ) -> List[Tuple[int, int]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞—Ä—Ç–µ —Å–¥–≤–∏–≥–æ–≤.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞.
        """
        n = len(shift_map)
        if n == 0:
            return [(0, len(text))]

        # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ: –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –±–µ—Ä—ë–º –º–æ–¥—É –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏
        smooth_window = 15
        smoothed = []
        for i in range(n):
            start = max(0, i - smooth_window // 2)
            end = min(n, i + smooth_window // 2 + 1)
            neighborhood = shift_map[start:end]
            mode = Counter(neighborhood).most_common(1)[0][0]
            smoothed.append(mode)

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫–∏ —Å–º–µ–Ω—ã
        boundaries = []
        seg_start = 0
        current_shift = smoothed[0]

        for i in range(1, n):
            if smoothed[i] != current_shift:
                boundaries.append((seg_start, i))
                seg_start = i
                current_shift = smoothed[i]

        boundaries.append((seg_start, n))

        # –§–∏–ª—å—Ç—Ä—É–µ–º: —Å–ª–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å —Å–æ—Å–µ–¥—è–º–∏
        min_segment = 15
        merged = []
        for start, end in boundaries:
            if end - start < min_segment and merged:
                prev_start, _ = merged[-1]
                merged[-1] = (prev_start, end)
            else:
                merged.append((start, end))

        if not merged:
            return [(0, len(text))]

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã: –∏—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –ø—Ä–æ–±–µ–ª/–∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        adjusted = []
        for i, (start, end) in enumerate(merged):
            # –ù–∞—á–∞–ª–æ: —Å–¥–≤–∏–≥–∞–µ–º –∫ –Ω–∞—á–∞–ª—É —Å–ª–æ–≤–∞
            if i > 0 and start > 0:
                # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –ø—Ä–æ–±–µ–ª/–∑–Ω–∞–∫ –Ω–∞–∑–∞–¥ (–¥–æ 5 —Å–∏–º–≤–æ–ª–æ–≤)
                for delta in range(min(5, start)):
                    if text[start - delta] in ' .,!?;:\n\t':
                        start = start - delta + 1
                        break
            # –ö–æ–Ω–µ—Ü: —Å–¥–≤–∏–≥–∞–µ–º –∫ –∫–æ–Ω—Ü—É —Å–ª–æ–≤–∞
            if i < len(merged) - 1 and end < len(text):
                for delta in range(min(5, len(text) - end)):
                    if text[end + delta] in ' .,!?;:\n\t':
                        end = end + delta
                        break
            adjusted.append((start, end))

        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        final = [adjusted[0]]
        for start, end in adjusted[1:]:
            prev_start, prev_end = final[-1]
            if start < prev_end:
                start = prev_end
            if start < end:
                final.append((start, end))

        return final


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# UI (Rich / Fallback)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class UI:
    def __init__(self):
        self.c = Console() if HAS_RICH else None

    def header(self):
        if self.c:
            self.c.print(Panel(
                "[bold cyan]CAESAR CRACKER ‚Äî ULTIMATE EDITION[/bold cyan]\n"
                "[dim]Chi¬≤ ‚Ä¢ –ë–∏–≥—Ä–∞–º–º—ã ‚Ä¢ –°—Ç–µ–º–º–∏–Ω–≥ ‚Ä¢ –°–º–µ—à–∞–Ω–Ω—ã–µ —à–∏—Ñ—Ä—ã[/dim]",
                border_style="cyan", box=box.DOUBLE
            ))
            self.c.print()
        else:
            print("=" * 70)
            print("  CAESAR CRACKER ‚Äî ULTIMATE EDITION")
            print("=" * 70)
            print()

    def info(self, dict_size: int, is_plain: bool, lang_name: str = ""):
        if self.c:
            status = "[green]‚úì –¢–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π (–Ω–µ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω)[/green]" if is_plain else "[yellow]üîê –¢–µ–∫—Å—Ç –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω[/yellow]"
            self.c.print(Panel(
                f"üìñ –°–ª–æ–≤–∞—Ä—å: [bold]{dict_size:,}[/bold] —Å–ª–æ–≤\n"
                f"üåê –Ø–∑—ã–∫: [bold]{lang_name}[/bold]\n"
                f"üìä –°—Ç–∞—Ç—É—Å: {status}",
                title="[bold]–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è[/bold]", border_style="blue"
            ))
            self.c.print()
        else:
            status = "–æ—Ç–∫—Ä—ã—Ç—ã–π" if is_plain else "–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω"
            print(f"–°–ª–æ–≤–∞—Ä—å: {dict_size:,} —Å–ª–æ–≤ | –Ø–∑—ã–∫: {lang_name} | –°—Ç–∞—Ç—É—Å: {status}")
            print()

    def result_single(self, best: ShiftResult, top5: List[ShiftResult]):
        if self.c:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –±–µ–∑ —Ä–∞–º–∫–∏, –ª–µ–≥–∫–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å
            self.c.print()
            self.c.print("[bold green]üí¨ –†–ê–°–®–ò–§–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:[/bold green]")
            self.c.print()
            self.c.print(best.text)
            self.c.print()

            # –ú–µ—Ç—Ä–∏–∫–∏
            self.c.print(
                f"[dim]üîë –ö–ª—é—á: [bold yellow]{best.shift}[/bold yellow]  "
                f"üìä {self._conf_colored(best.confidence)}  "
                f"üìñ {best.matches}/{best.total_words} —Å–ª–æ–≤  "
                f"Chi¬≤={best.chi_sq:.0f}  "
                f"–ë–∏–≥—Ä.: {best.bigram_score:.0%}  "
                f"–°–ª–æ–≤.: {best.dict_score:.0%}  "
                f"–°—Ç–µ–º.: {best.stem_score:.0%}[/dim]"
            )
            self.c.print()

            # –¢–æ–ø-5
            t5 = Table(
                box=box.SIMPLE, show_header=True,
                header_style="bold", title="[bold]–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã[/bold]"
            )
            t5.add_column("#", width=4)
            t5.add_column("–ö–ª—é—á", width=6)
            t5.add_column("–î–æ—Å—Ç–æ–≤.", width=10)
            t5.add_column("–¢–µ–∫—Å—Ç")

            for i, r in enumerate(top5, 1):
                marker = "‚≠ê" if i == 1 else str(i)
                preview = r.text[:60] + "‚Ä¶" if len(r.text) > 60 else r.text
                t5.add_row(marker, str(r.shift), self._conf_colored(r.confidence), preview)

            self.c.print(t5)
        else:
            print(f"\nüí¨ –†–ê–°–®–ò–§–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:")
            print(best.text)
            print(f"\nüîë –ö–ª—é—á: {best.shift}  –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {best.confidence:.1f}%  "
                  f"–°–ª–æ–≤: {best.matches}/{best.total_words}")
            print(f"Chi¬≤={best.chi_sq:.1f}  –ë–∏–≥—Ä.={best.bigram_score:.0%}  "
                  f"–°–ª–æ–≤.={best.dict_score:.0%}  –°—Ç–µ–º.={best.stem_score:.0%}")
            print("\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:")
            for i, r in enumerate(top5, 1):
                m = "‚≠ê" if i == 1 else f"{i}."
                p = r.text[:60] + "‚Ä¶" if len(r.text) > 60 else r.text
                print(f"  {m} –∫–ª—é—á={r.shift} ({r.confidence:.0f}%) {p}")

    def result_mixed(self, segments: List[Segment]):
        keys = [s.best_result.shift for s in segments]
        is_mixed = len(set(keys)) > 1

        full_text = ''.join(s.text for s in segments)
        avg_conf = sum(s.best_result.confidence for s in segments) / len(segments)

        if self.c:
            if is_mixed:
                self.c.print(Panel(
                    f"[bold yellow]‚ö†Ô∏è  –°–ú–ï–®–ê–ù–ù–´–ô –®–ò–§–†: {len(set(keys))} —Ä–∞–∑–Ω—ã—Ö –∫–ª—é—á–µ–π[/bold yellow]\n"
                    f"–ö–ª—é—á–∏: [bold]{', '.join(str(k) for k in keys)}[/bold]",
                    border_style="yellow", box=box.HEAVY
                ))
                self.c.print()

            tbl = Table(
                box=box.ROUNDED, show_header=True,
                header_style="bold magenta", title="[bold]–°–µ–≥–º–µ–Ω—Ç—ã[/bold]"
            )
            tbl.add_column("#", width=4, style="cyan")
            tbl.add_column("–ö–ª—é—á", width=6, style="yellow")
            tbl.add_column("–î–æ—Å—Ç–æ–≤.", width=10)
            tbl.add_column("–°–ª–æ–≤–∞", width=8, style="blue")
            tbl.add_column("–¢–µ–∫—Å—Ç")

            for i, seg in enumerate(segments, 1):
                r = seg.best_result
                preview = seg.text[:50] + "‚Ä¶" if len(seg.text) > 50 else seg.text
                tbl.add_row(
                    str(i), str(r.shift),
                    self._conf_colored(r.confidence),
                    f"{r.matches}/{r.total_words}",
                    preview
                )

            self.c.print(tbl)
            self.c.print()

            self.c.print("[bold green]üí¨ –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢:[/bold green]")
            self.c.print()
            self.c.print(full_text)
            self.c.print()
        else:
            if is_mixed:
                print(f"\n‚ö†Ô∏è  –°–ú–ï–®–ê–ù–ù–´–ô –®–ò–§–†: –∫–ª—é—á–∏ {keys}")
            for i, seg in enumerate(segments, 1):
                r = seg.best_result
                print(f"  –°–µ–≥–º–µ–Ω—Ç {i}: –∫–ª—é—á={r.shift} ({r.confidence:.0f}%) {seg.text[:60]}")
            print(f"\n–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç ({avg_conf:.0f}%):\n{full_text}")

    def _conf_colored(self, conf: float) -> str:
        if not self.c:
            return f"{conf:.1f}%"
        if conf >= 80:
            return f"[bold green]{conf:.1f}%[/bold green]"
        elif conf >= 50:
            return f"[yellow]{conf:.1f}%[/yellow]"
        else:
            return f"[red]{conf:.1f}%[/red]"

    def ask_multiline(self, prompt: str) -> str:
        """–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –≤–≤–æ–¥: –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ Ctrl+D –∑–∞–≤–µ—Ä—à–∞–µ—Ç"""
        if self.c:
            self.c.print(f"[bold yellow]{prompt}[/bold yellow]")
            self.c.print("[dim](–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ = –∫–æ–Ω–µ—Ü –≤–≤–æ–¥–∞)[/dim]")
        else:
            print(f"{prompt}")
            print("(–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ = –∫–æ–Ω–µ—Ü –≤–≤–æ–¥–∞)")

        lines = []
        try:
            while True:
                line = input()
                if line == '':
                    break
                lines.append(line)
        except EOFError:
            pass
        return '\n'.join(lines)

    def confirm(self, question: str) -> bool:
        if self.c:
            return Confirm.ask(f"[bold]{question}[/bold]")
        return input(f"{question} (y/n): ").strip().lower() in ('y', '–¥', '–¥–∞')


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog='caesar',
        description='Caesar Cipher Cracker ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—à–∏—Ñ—Ä–æ–≤–∫–∞ —à–∏—Ñ—Ä–∞ –¶–µ–∑–∞—Ä—è',
    )
    p.add_argument('text', nargs='*', help='–ó–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç')
    p.add_argument('-r', '--raw', action='store_true',
                   help='–í—ã–≤–µ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—É–¥–æ–±–Ω–æ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ pipe)')
    p.add_argument('-m', '--mixed', action='store_true',
                   help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–º–µ—à–∞–Ω–Ω—ã–π —à–∏—Ñ—Ä')
    p.add_argument('-l', '--lang', choices=['ru', 'en'],
                   help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞—Ç—å —è–∑—ã–∫ (–∏–Ω–∞—á–µ –∞–≤—Ç–æ)')
    return p.parse_args()


def run():
    args = parse_args()
    raw = args.raw

    analyzer = Analyzer()
    detector = MixedDetector()

    # –í–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞
    if args.text:
        text = ' '.join(args.text)
        auto = True
    elif not sys.stdin.isatty():
        text = sys.stdin.read().strip()
        auto = True
    else:
        if raw:
            print("–û—à–∏–±–∫–∞: –≤ —Ä–µ–∂–∏–º–µ --raw –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º –∏–ª–∏ —á–µ—Ä–µ–∑ pipe", file=sys.stderr)
            sys.exit(1)
        ui = UI()
        ui.header()
        text = ui.ask_multiline("–í–≤–µ–¥–∏—Ç–µ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
        auto = False

    if not text or text.lower() in ('exit', 'quit', 'q'):
        return

    forced_lang = args.lang
    bilingual = (not forced_lang) and analyzer.is_bilingual(text)

    # UI –¥–ª—è –Ω–µ-raw —Ä–µ–∂–∏–º–∞
    if not raw:
        if auto:
            ui = UI()
            ui.header()

    if bilingual:
        _crack_bilingual(text, analyzer, detector, args, raw, ui if not raw else None)
    else:
        lang = forced_lang or analyzer.detect_language(text)
        _crack_single_lang(text, lang, analyzer, detector, args, raw, ui if not raw else None, auto)


def _crack_bilingual(text, analyzer, detector, args, raw, ui):
    """–†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —è–∑—ã–∫–∞–º, –¥–µ—à–∏—Ñ—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç —Å–≤–æ–∏–º –∞–ª—Ñ–∞–≤–∏—Ç–æ–º"""
    lang_segments = split_by_language(text)
    parts = []

    for lseg in lang_segments:
        results = analyzer.crack(lseg.text, lseg.lang)
        best = results[0]
        parts.append((lseg, best))

    full_text = ''.join(best.text for _, best in parts)

    if raw:
        print(full_text)
        return

    langs = set(ls.lang for ls in lang_segments)
    lang_name = "Russian + English" if len(langs) > 1 else ("–†—É—Å—Å–∫–∏–π" if 'ru' in langs else "English")
    ui.info(len(analyzer.dict), False, lang_name)

    if ui.c:
        ui.c.print()
        ui.c.print("[bold green]üí¨ –†–ê–°–®–ò–§–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:[/bold green]")
        ui.c.print()
        ui.c.print(full_text)
        ui.c.print()
        for lseg, best in parts:
            lang_tag = "RU" if lseg.lang == 'ru' else "EN"
            ui.c.print(
                f"[dim][{lang_tag}] –∫–ª—é—á={best.shift}  "
                f"{ui._conf_colored(best.confidence)}  "
                f"{best.matches}/{best.total_words} —Å–ª–æ–≤[/dim]"
            )
    else:
        print(f"\nüí¨ –†–ê–°–®–ò–§–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:")
        print(full_text)
        for lseg, best in parts:
            lang_tag = "RU" if lseg.lang == 'ru' else "EN"
            print(f"  [{lang_tag}] –∫–ª—é—á={best.shift} ({best.confidence:.0f}%) {best.matches}/{best.total_words} —Å–ª–æ–≤")


def _crack_single_lang(text, lang, analyzer, detector, args, raw, ui, auto=True):
    """–î–µ—à–∏—Ñ—Ä–æ–≤–∫–∞ –æ–¥–Ω–æ—è–∑—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if raw:
        results = analyzer.crack(text, lang)
        best = results[0]
        if best.confidence < 60 and len(text) > 60:
            segments = detector.detect(text)
            keys = set(s.best_result.shift for s in segments)
            if len(keys) > 1:
                print(''.join(s.text for s in segments))
                return
        print(best.text)
        return

    lang_name = "–†—É—Å—Å–∫–∏–π" if lang == 'ru' else "English"
    is_plain = analyzer.is_already_plaintext(text)
    ui.info(len(analyzer.dict), is_plain, lang_name)

    if is_plain:
        if auto:
            results = analyzer.crack(text, lang)
            ui.result_single(results[0], results[:5])
            return
        else:
            proceed = ui.confirm("–¢–µ–∫—Å—Ç –ø–æ—Ö–æ–∂ –Ω–∞ –Ω–µ–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?")
            if not proceed:
                return

    results = analyzer.crack(text, lang)
    best = results[0]

    if (args.mixed or (best.confidence < 60 and len(text) > 60)):
        segments = detector.detect(text)
        keys = set(s.best_result.shift for s in segments)
        if len(keys) > 1:
            ui.result_mixed(segments)
            return

    ui.result_single(best, results[:5])


if __name__ == '__main__':
    try:
        run()
    except KeyboardInterrupt:
        print("\nüëã")
    except Exception as e:
        print(f"\n‚ùå {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
